{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hennecol\\AppData\\Local\\Temp\\ipykernel_9460\\1838224339.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from moviepy.editor import VideoFileClip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"lunar-airport-418323\"  # @param {type:\"string\"}\n",
    "LOCATION = \"asia-southeast1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GenerationConfig with desired parameters\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.0,  # Adjust temperature between 0.0 (deterministic) and 1.0 (more random)\n",
    "    #top_p=0.3,  # Adjust top_p between 0.0 (likely tokens) and 1.0 (considers all tokens)\n",
    "    #top_k= 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://datavid2/part 2 - 3 video 1_Trim.mp4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define uri for the full video\n",
    "video_name = \"part 2 - 3 video 1_Trim\" #without the .mp4 extension\n",
    "video_bucket_nb = 2 #1 or 2\n",
    "\n",
    "video_uri = f\"gs://datavid{video_bucket_nb}/{video_name}.mp4\"\n",
    "video_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, there is a situation that will end in an accident. A white car is driving in the same trajectory as the camera's car, but in the opposite direction. The white car is on the wrong side of the road and is driving towards the camera's car. If the two cars continue on their current paths, they will collide head-on."
     ]
    }
   ],
   "source": [
    "#Test with full video\n",
    "prompt = \"\"\"\n",
    "This video is a first person view from inside a moving car, in a country where people drive on the left. Answer the following questions using the video only:\n",
    "You are the driver of the camera's car, can you spot a situation that will end in an accident where another element is in the same trajectory as your car? If yes:\n",
    "Say yes, describe the situation and give the elements involved.\n",
    "If no:\n",
    "Say no and describe what is shown in the video.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This video is a first person view from inside a moving car, in a country where people drive on the left. If there is a situation where an accident is about to occur, the video is cut just before the accident occurs, so the accident situation is just before the end of the video.\n",
    "Answer the following questions using the video only:\n",
    "You are the driver of the camera's car, can you spot a situation where an accident is about to occur with your car? If yes:\n",
    "Describe the situation and give the elements involved and where they come from in relation to your car.\n",
    "\"\"\"\n",
    "\n",
    "'''This video is taken from inside a car, from the front passenger seat, in a country where people drive on the left. If there is a situation where an accident is about to occur, the video is cut just before the accident occurs, so the accident situation is just before the end of the video.\n",
    "Answer the following questions using the video only:\n",
    "Can you spot a situation where an accident is about to occur in the final moments? If yes:\n",
    "Describe the situation and give the elements involved.'''\n",
    "\n",
    "'''This video is a first person view from inside a moving car, in a country where people drive on the left. If there is a situation where an accident is about to occur, the video is cut just before the accident occurs, so the accident situation is just before the end of the video.\n",
    "Answer the following questions using the video only:\n",
    "Can you spot a situation where an accident is about to occur with the camera? If yes:\n",
    "Describe the situation, give the elements involved and where they come from in relation to the camera.'''\n",
    "\n",
    "\n",
    "video = Part.from_uri(video_uri, mime_type=\"video/mp4\")\n",
    "contents = [video, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, generation_config=generation_config, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we cut the video and keep only the part after risk_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to trim the video after risk_time\n",
    "# Define the paths\n",
    "csv_path = \"C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/2 analysis and first preprocessing/liste_videos_pour_script - Copy.csv\"  # Adjust this to the correct path\n",
    "video_source_folder = \"C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/1 experiment/final/datavid1\"\n",
    "video_dest_folder = \"C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/1 experiment/final/datavid1/videos_cuts\"\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "if not os.path.exists(video_dest_folder):\n",
    "    os.makedirs(video_dest_folder)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    video_name = row['nom']\n",
    "    risk_time = row['risk_time']\n",
    "\n",
    "    # Check if risk_time is a number (ignoring NaN or non-numeric values)\n",
    "    if pd.notna(risk_time) and isinstance(risk_time, (int, float, str)):\n",
    "        try:\n",
    "            risk_time = float(risk_time)  # Ensure risk_time is in seconds\n",
    "            # Define the source and destination paths for the video\n",
    "            source_path = video_source_folder + \"/\" + video_name\n",
    "            dest_path = video_dest_folder + \"/\" + f\"cut_{video_name}\"\n",
    "            print(source_path, dest_path)\n",
    "\n",
    "            # Load the video\n",
    "            video = VideoFileClip(source_path)\n",
    "\n",
    "            # Cut the video from risk_time to the end\n",
    "            cut_video = video.subclip(risk_time, video.duration)\n",
    "\n",
    "            # Save the cut video with a video format extension\n",
    "            cut_video.write_videofile(dest_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "            # Close the video file to free up resources\n",
    "            video.close()\n",
    "            cut_video.close()\n",
    "\n",
    "            print(f\"Video {video_name} cut and saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://datavid1/12 crashes 4.mp4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define uri for the trimmed video\n",
    "video_cut_name = \"12 crashes 4\" #without the .mp4 extension\n",
    "video_bucket_nb = 1 #1 or 2\n",
    "\n",
    "video_cut_uri = f\"gs://datavid{video_bucket_nb}/{video_cut_name}.mp4\"\n",
    "video_cut_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There is a situation where an accident could happen with the car of the person filming.\n",
      "At 0:19 seconds into the video, a silver car is seen approaching from the right, on the wrong side of the road. It is likely that this car will collide with the car of the person filming, as it is not possible for both cars to pass each other on the narrow road."
     ]
    }
   ],
   "source": [
    "#Test with trimmed video\n",
    "prompt = \"\"\"\n",
    "This video is a first person view from inside a moving car, in a country where people drive on the left.\n",
    "You are the driver of the camera's car, can you spot a situation where an accident could happen with your car? If yes:\n",
    "Describe the situation, give the time of the video it appears (in seconds from the beginning) and give the elements involved.\n",
    "\"\"\"\n",
    "#This video is taken from inside a car, from the front passenger seat, in a country where people drive on the left. This video shows a situation where an accident is about to occur. Can you describe the situation shown in the video ?\n",
    "\n",
    "video = Part.from_uri(video_cut_uri, mime_type=\"video/mp4\")\n",
    "contents = [video, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, generation_config=generation_config, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate a description of every videos in the bucket folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 Request contains an invalid argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hennecol\\Documents\\FMRI experiment\\FINAL FOLDER\\Lucas\\virtual_environnement\\carrs-q_projectv2\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:170\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m     prefetch_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callable_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_prefetch_first_result_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_StreamingResponseIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefetch_first_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefetch_first\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\hennecol\\Documents\\FMRI experiment\\FINAL FOLDER\\Lucas\\virtual_environnement\\carrs-q_projectv2\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:92\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__init__\u001b[1;34m(self, wrapped, prefetch_first_result)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefetch_first_result:\n\u001b[1;32m---> 92\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stored_first_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# It is possible the wrapped method isn't an iterable (a grpc.Call\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# for instance). If this happens don't store the first result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hennecol\\Documents\\FMRI experiment\\FINAL FOLDER\\Lucas\\virtual_environnement\\carrs-q_projectv2\\Lib\\site-packages\\grpc\\_channel.py:542\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hennecol\\Documents\\FMRI experiment\\FINAL FOLDER\\Lucas\\virtual_environnement\\carrs-q_projectv2\\Lib\\site-packages\\grpc\\_channel.py:968\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"Request contains an invalid argument.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.217.167.106:443 {grpc_message:\"Request contains an invalid argument.\", grpc_status:3, created_time:\"2024-04-09T05:35:02.6126552+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m video \u001b[38;5;241m=\u001b[39m Part\u001b[38;5;241m.\u001b[39mfrom_uri(video_uri, mime_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo/mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m contents \u001b[38;5;241m=\u001b[39m [video, prompt]\n\u001b[1;32m---> 16\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmultimodal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m responses\n",
      "File \u001b[1;32mc:\\Users\\hennecol\\Documents\\FMRI experiment\\FINAL FOLDER\\Lucas\\virtual_environnement\\carrs-q_projectv2\\Lib\\site-packages\\vertexai\\generative_models\\_generative_models.py:501\u001b[0m, in \u001b[0;36m_GenerativeModel._generate_content_streaming\u001b[1;34m(self, contents, generation_config, safety_settings, tools)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates content.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    A stream of GenerationResponse objects\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    495\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    496\u001b[0m     contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[0;32m    497\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m    498\u001b[0m     safety_settings\u001b[38;5;241m=\u001b[39msafety_settings,\n\u001b[0;32m    499\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    500\u001b[0m )\n\u001b[1;32m--> 501\u001b[0m response_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response_stream:\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_response(chunk)\n",
      "File \u001b[1;32mc:\\Users\\hennecol\\Documents\\FMRI experiment\\FINAL FOLDER\\Lucas\\virtual_environnement\\carrs-q_projectv2\\Lib\\site-packages\\google\\cloud\\aiplatform_v1beta1\\services\\prediction_service\\client.py:2207\u001b[0m, in \u001b[0;36mPredictionServiceClient.stream_generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   2204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m   2206\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 2207\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2212\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\hennecol\\Documents\\FMRI experiment\\FINAL FOLDER\\Lucas\\virtual_environnement\\carrs-q_projectv2\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hennecol\\Documents\\FMRI experiment\\FINAL FOLDER\\Lucas\\virtual_environnement\\carrs-q_projectv2\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:174\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StreamingResponseIterator(\n\u001b[0;32m    171\u001b[0m         result, prefetch_first_result\u001b[38;5;241m=\u001b[39mprefetch_first\n\u001b[0;32m    172\u001b[0m     )\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 Request contains an invalid argument."
     ]
    }
   ],
   "source": [
    "csv_path = \"C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/2 analysis and first preprocessing/liste_videos_pour_script_and_infos2.csv\"  # Adjust this to the correct path\n",
    "video_bucket_nb = 1 #1 or 2\n",
    "\n",
    "prompt = \"\"\"This video is a first person view from inside a moving car, in a country where people drive on the left. Answer the following questions using the video only:\n",
    "You are the driver of the camera's car, can you spot a situation that will end in an accident where another element is in the same trajectory as your car? If yes:\n",
    "Say yes, describe the situation and give the elements involved.\n",
    "If no:\n",
    "Say no and describe what is shown in the video.\"\"\" #prompt of the column description2\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "row = df.head(1)\n",
    "video_uri = f\"gs://datavid{video_bucket_nb}/{video_name}\"\n",
    "video = Part.from_uri(video_uri, mime_type=\"video/mp4\")\n",
    "contents = [video, prompt]\n",
    "responses = [response.text for response in multimodal_model.generate_content(contents, generation_config=generation_config, stream=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0 processed\n",
      "Line 1 processed\n",
      "Line 2 processed\n",
      "Line 3 processed\n",
      "Line 4 processed\n",
      "Line 5 processed\n",
      "Line 6 processed\n",
      "Line 7 processed\n",
      "Line 8 processed\n",
      "Line 9 processed\n",
      "Line 10 processed\n",
      "Line 11 processed\n",
      "Line 12 processed\n",
      "Line 13 processed\n",
      "Line 14 processed\n",
      "Line 15 processed\n",
      "Line 16 processed\n",
      "Line 17 processed\n",
      "Line 18 processed\n",
      "Line 19 processed\n",
      "Line 20 processed\n",
      "Line 21 processed\n",
      "Line 22 processed\n",
      "Line 23 processed\n",
      "Line 24 processed\n",
      "Line 25 processed\n",
      "Line 26 processed\n",
      "Line 27 processed\n",
      "Line 28 processed\n",
      "Line 29 processed\n",
      "Line 30 processed\n",
      "Line 31 processed\n",
      "Line 32 processed\n",
      "Line 33 processed\n",
      "Line 34 processed\n"
     ]
    }
   ],
   "source": [
    "## Code to generate descriptions of the videos and store them in a .csv file\n",
    "\n",
    "csv_path = \"C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/2 analysis and first preprocessing/liste_videos_pour_script_and_infos2.csv\"  # Adjust this to the correct path\n",
    "video_bucket_nb = 1 #1 or 2\n",
    "\n",
    "prompt = \"\"\"This video is a first person view from inside a moving car, in a country where people drive on the left. Answer the following questions using the video only:\n",
    "You are the driver of the camera's car, can you spot a situation that will end in an accident where another element is in the same trajectory as your car? If yes:\n",
    "Describe the situation and give the elements involved.\n",
    "If no:\n",
    "Say no and describe what is shown in the video.\"\"\" #prompt of the column description2\n",
    "\n",
    "\n",
    "#Prompt of the column \"description\"\n",
    "# \"\"\"\n",
    "# This video is a first person view from inside a moving car, in a country where people drive on the left. If there is a situation where an accident is about to occur, the video is cut just before the accident occurs, so the accident situation is just before the end of the video.\n",
    "# Answer the following questions using the video only:\n",
    "# You are the driver of the camera's car, can you spot a situation where an accident is about to occur with your car? If yes:\n",
    "# Describe the situation and give the elements involved.\n",
    "# \"\"\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    video_name = row['nom']\n",
    "    video_uri = f\"gs://datavid{video_bucket_nb}/{video_name}\"\n",
    "    video = Part.from_uri(video_uri, mime_type=\"video/mp4\")\n",
    "    contents = [video, prompt]\n",
    "    responses = [response.text for response in multimodal_model.generate_content(contents, generation_config=generation_config, stream=True)]\n",
    "    # responses = multimodal_model.generate_content(contents, generation_config=generation_config, stream=True)\n",
    "    # for response in responses:\n",
    "    #     print(response.text, end=\"\")\n",
    "\n",
    "    response_text = ''.join(responses)\n",
    "    df.at[index, 'description3'] = response_text # Update the 'description2' column for the current row\n",
    "    print(f'Line {index} processed')\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New method : extract two frames from the video at risk_time and (risk_time + end_time)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame à 7.60 secondes sauvegardée.\n",
      "Frame à 8.03 secondes sauvegardée.\n"
     ]
    }
   ],
   "source": [
    "## Function to extract frames from the video:\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "def extract_frames(video_path, video_name, time, output_folder):\n",
    "    # Ouvrir la vidéo\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Vérifier si la vidéo a été chargée correctement\n",
    "    if not cap.isOpened():\n",
    "        print(\"Erreur : Impossible d'ouvrir la vidéo.\")\n",
    "        return\n",
    "    \n",
    "    # Obtenir le nombre total de frames et le fps (frames par seconde)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Calculer les numéros des frames à extraire\n",
    "    frame_num_time_sec = int(time * fps)  # Frame à la seconde time\n",
    "    half_way_frame = int((((total_frames / fps) + time)/ 2) * fps)  # Frame à la moitié du temps entre time et la fin\n",
    "    \n",
    "    # Définir les moments d'extraction\n",
    "    moments_to_extract = [frame_num_time_sec, half_way_frame]\n",
    "    \n",
    "    for count, moment in enumerate(moments_to_extract, start=1):\n",
    "        if moment >= total_frames:\n",
    "            print(f\"Le moment d'extraction {moment/fps:.2f} secondes dépasse la durée de la vidéo.\")\n",
    "            continue\n",
    "        # Se positionner sur la frame à extraire\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, moment)\n",
    "        \n",
    "        # Lire la frame\n",
    "        success, frame = cap.read()\n",
    "        if success:\n",
    "            # Sauvegarder la frame extraite\n",
    "            output_path = output_folder + \"/\" + video_name.split(\".\")[0] + f\"frame_{count}.jpg\"\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            print(f\"Frame à {moment/fps:.2f} secondes sauvegardée.\")\n",
    "        else:\n",
    "            print(f\"Erreur lors de l'extraction de la frame à {moment/fps:.2f} secondes.\")\n",
    "    \n",
    "    # Libérer le capture vidéo\n",
    "    cap.release()\n",
    "\n",
    "# Remplacez 'chemin_vers_votre_video.mp4' par le chemin réel de votre vidéo\n",
    "video_path = 'C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/1 experiment/final/datavid1/1 weekly - 3.mp4'\n",
    "video_name = \"1 weekly - 3.mp4\"\n",
    "time = 7.6\n",
    "output_folder = \"C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/1 experiment/final/datavid1/video_frames\"\n",
    "extract_frames(video_path,video_name, time, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing video 15_low1_1.mp4: could not convert string to float: '-'\n",
      "Error processing video 15_low3_2.mp4: could not convert string to float: '-'\n",
      "Frame à 3.50 secondes sauvegardée.\n",
      "Frame à 3.93 secondes sauvegardée.\n",
      "Error processing video 11 controls 6.mp4: could not convert string to float: '-'\n",
      "Frame à 10.20 secondes sauvegardée.\n",
      "Frame à 12.67 secondes sauvegardée.\n",
      "Frame à 3.70 secondes sauvegardée.\n",
      "Frame à 4.50 secondes sauvegardée.\n",
      "Frame à 5.50 secondes sauvegardée.\n",
      "Frame à 6.97 secondes sauvegardée.\n",
      "Frame à 1.40 secondes sauvegardée.\n",
      "Frame à 8.17 secondes sauvegardée.\n",
      "Frame à 10.00 secondes sauvegardée.\n",
      "Frame à 16.10 secondes sauvegardée.\n",
      "Error processing video 15_low1_3.mp4: could not convert string to float: '-'\n",
      "Frame à 4.20 secondes sauvegardée.\n",
      "Frame à 4.67 secondes sauvegardée.\n",
      "Frame à 1.80 secondes sauvegardée.\n",
      "Frame à 3.27 secondes sauvegardée.\n",
      "Error processing video 14 controls countryside.mp4: could not convert string to float: '-'\n",
      "Frame à 5.90 secondes sauvegardée.\n",
      "Frame à 6.83 secondes sauvegardée.\n",
      "Frame à 2.00 secondes sauvegardée.\n",
      "Frame à 10.73 secondes sauvegardée.\n",
      "Frame à 7.60 secondes sauvegardée.\n",
      "Frame à 8.03 secondes sauvegardée.\n",
      "Frame à 5.10 secondes sauvegardée.\n",
      "Frame à 5.77 secondes sauvegardée.\n",
      "Error processing video 15_low2_3.mp4: could not convert string to float: '-'\n",
      "Frame à 5.10 secondes sauvegardée.\n",
      "Frame à 5.83 secondes sauvegardée.\n",
      "Error processing video 14 controls suburb.mp4: could not convert string to float: '-'\n",
      "Frame à 6.10 secondes sauvegardée.\n",
      "Frame à 6.87 secondes sauvegardée.\n",
      "Frame à 7.30 secondes sauvegardée.\n",
      "Frame à 7.83 secondes sauvegardée.\n",
      "Error processing video 6 - weekly 6.mp4: could not convert string to float: '-'\n",
      "Frame à 7.10 secondes sauvegardée.\n",
      "Frame à 7.87 secondes sauvegardée.\n",
      "Error processing video 15_low3_1.mp4: could not convert string to float: '-'\n",
      "Frame à 6.10 secondes sauvegardée.\n",
      "Frame à 6.70 secondes sauvegardée.\n",
      "Error processing video 10 controls 5.mp4: could not convert string to float: '-'\n",
      "Frame à 3.00 secondes sauvegardée.\n",
      "Frame à 4.07 secondes sauvegardée.\n",
      "Frame à 12.00 secondes sauvegardée.\n",
      "Frame à 14.13 secondes sauvegardée.\n",
      "Error processing video 15_low6_2.mp4: could not convert string to float: '-'\n",
      "Frame à 5.80 secondes sauvegardée.\n",
      "Frame à 6.40 secondes sauvegardée.\n",
      "Frame à 6.60 secondes sauvegardée.\n",
      "Frame à 7.33 secondes sauvegardée.\n",
      "Frame à 9.80 secondes sauvegardée.\n",
      "Frame à 10.83 secondes sauvegardée.\n",
      "Error processing video 10 controls 8.mp4: could not convert string to float: '-'\n",
      "Error processing video 15_low2_4.mp4: could not convert string to float: '-'\n"
     ]
    }
   ],
   "source": [
    "# Extract frames from all videos of a folder\n",
    "csv_path = \"C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/2 analysis and first preprocessing/liste_videos_pour_script_and_infos.csv\"  # Adjust this to the correct path\n",
    "video_source_folder = \"C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/1 experiment/final/datavid1\"\n",
    "video_dest_folder = \"C:/Users/hennecol/Documents/FMRI experiment/FINAL FOLDER/1 experiment/final/datavid1/video_frames\"\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "if not os.path.exists(video_dest_folder):\n",
    "    os.makedirs(video_dest_folder)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    video_name = row['nom']\n",
    "    risk_time = row['risk_time']\n",
    "\n",
    "        # Check if risk_time is a number (ignoring NaN or non-numeric values)\n",
    "    if pd.notna(risk_time) and isinstance(risk_time, (int, float, str)):\n",
    "        try:\n",
    "            risk_time = float(risk_time)  # Ensure risk_time is in seconds\n",
    "            # Define the source and destination paths for the video\n",
    "            video_path = video_source_folder + \"/\" + video_name\n",
    "            extract_frames(video_path, video_name, risk_time, video_dest_folder)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define uri for the frames\n",
    "video_name = \"15_low5_1\" #without the .mp4 extension\n",
    "frame_bucket_nb = 1 #1 or 2\n",
    "\n",
    "frame_1_uri = f\"gs://datapict{frame_bucket_nb}/{video_name}frame_1.jpg\"\n",
    "frame_2_uri = f\"gs://datapict{frame_bucket_nb}/{video_name}frame_2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, there is a situation that will end in an accident. A bus is coming in the opposite direction and it is in the same trajectory as the car. The bus is on the left side of the road, and the car is on the right side of the road. If the bus continues to go straight, it will hit the car."
     ]
    }
   ],
   "source": [
    "#Test with the two following frames\n",
    "prompt = \"\"\"\n",
    "These two images taken within seconds of each other are a first person view from inside a moving car, in a country where people drive on the left.\n",
    "You are the driver of the camera's car, can you spot a situation that will end in an accident where another element is in the same trajectory as your car? If yes:\n",
    "Say yes, describe the situation and give the elements involved.\n",
    "If no:\n",
    "Say no and describe what is shown in the video.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "These two images taken within seconds of each other are a first person view from inside a moving car, in a country where people drive on the left.\n",
    "You are the driver of the camera's car, can you spot a situation where an accident could happen with your car? If yes:\n",
    "Describe the situation and give the elements involved.\n",
    "'''\n",
    "#This video is taken from inside a car, from the front passenger seat, in a country where people drive on the left. This video shows a situation where an accident is about to occur. Can you describe the situation shown in the video ?\n",
    "\n",
    "frame_1 = Part.from_uri(frame_1_uri, mime_type=\"image/jpeg\")\n",
    "frame_2 = Part.from_uri(frame_2_uri, mime_type=\"image/jpeg\")\n",
    "contents = [frame_1, frame_2, prompt]\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, generation_config=generation_config, stream=True)\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carrs-q_projectv2",
   "language": "python",
   "name": "carrs-q_projectv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
